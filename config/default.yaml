model:
  name: "google/gemma-2-2b-it"
  # Optional parameters. If omitted, defaults will be used.
  pad_token: "<pad>"
  chat_template: |
    {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}
    {% for message in messages %}
    {{ '<start_of_turn>' + message['role'] + '\n' + message['content'] | trim + '<end_of_turn><eos>\n' }}
    {% endfor %}
    {% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}
  special_tokens:
    - "<pad>"
    - "<eos>"
    - "<tools>"
    - "</tools>"
    - "<think>"
    - "</think>"
    - "<tool_call>"
    - "</tool_call>"
    - "<tool_response>"
    - "</tool_response>"
  # Do NOT include hf_token here. Provide your Hugging Face token in config/token.yaml or via the HF_TOKEN environment variable.

dataset_name: "Jofthomas/hermes-function-calling-thinking-V1"
test_size: 0.1
output_dir: "output_model"
push_to_hub: false
username: "your_username"  # Only used if push_to_hub is true

training_args:
  output_dir: "output_model"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  save_strategy: "no"
  eval_strategy: "epoch"
  logging_steps: 5
  learning_rate: 0.0001
  max_grad_norm: 1.0
  weight_decay: 0.1
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  bf16: true
  hub_private_repo: false
  push_to_hub: false
  num_train_epochs: 1
  gradient_checkpointing: true
  packing: true
  max_seq_length: 1500
